{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99676749-2d77-45be-8e0d-fb01ec1c718e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\ACER\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag-of-Words SVM Accuracy: 0.86\n",
      "Bag-of-Words Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.82      0.85       999\n",
      "           1       0.83      0.90      0.86      1001\n",
      "\n",
      "    accuracy                           0.86      2000\n",
      "   macro avg       0.86      0.86      0.86      2000\n",
      "weighted avg       0.86      0.86      0.86      2000\n",
      "\n",
      "TF-IDF SVM Accuracy: 0.88\n",
      "TF-IDF Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.85      0.87       999\n",
      "           1       0.86      0.91      0.88      1001\n",
      "\n",
      "    accuracy                           0.88      2000\n",
      "   macro avg       0.88      0.88      0.88      2000\n",
      "weighted avg       0.88      0.88      0.88      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "# Ortak ön işleme adımları\n",
    "def to_lowercase(text):\n",
    "    \"\"\"\n",
    "    Metni küçük harfe çevirir.\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "def remove_punctuation(text):\n",
    "    \"\"\"\n",
    "    Metindeki noktalama işaretlerini kaldırır.\n",
    "    \"\"\"\n",
    "    return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "def remove_numbers(text, replace_with_tag=False):\n",
    "    \"\"\"\n",
    "    Metindeki sayıları kaldırır veya <number> etiketiyle değiştirir.\n",
    "    replace_with_tag=True olduğunda sayılar yerine <number> eklenir.\n",
    "    \"\"\"\n",
    "    if replace_with_tag:\n",
    "        return re.sub(r'\\d+', '<number>', text)\n",
    "    else:\n",
    "        return re.sub(r'\\d+', '', text)\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    \"\"\"\n",
    "    Stopwords'leri metinden çıkarır.\n",
    "    \"\"\"\n",
    "    stop_words = set(stopwords.words(lang))\n",
    "    return \" \".join([word for word in text.split() if word not in stop_words])\n",
    "def apply_stemming(text):\n",
    "    \"\"\"\n",
    "    Metindeki kelimeleri köklerine indirir (Stemming).\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return \" \".join([stemmer.stem(word) for word in text.split()])\n",
    "def apply_lemmatization(text):\n",
    "    \"\"\"\n",
    "    Metindeki kelimeleri köklerine indirir (Lemmatization).\n",
    "    \"\"\"\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return \" \".join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "def preprocess_text(\n",
    "    text,\n",
    "    lowercase=True,\n",
    "    remove_punct=True,\n",
    "    remove_nums=True,\n",
    "    replace_nums_with_tag=True,\n",
    "    remove_sw=True,\n",
    "    apply_stem=False,\n",
    "    apply_lemma=True,\n",
    "    lang='english'\n",
    "):\n",
    "    \"\"\"\n",
    "    Metni temizler ve ön işlemden geçirir.\n",
    "    \"\"\"\n",
    "    if lowercase:\n",
    "        text = to_lowercase(text)\n",
    "    if remove_punct:\n",
    "        text = remove_punctuation(text)\n",
    "    if remove_nums:\n",
    "        text = remove_numbers(text, replace_with_tag=replace_nums_with_tag)\n",
    "    if remove_sw:\n",
    "        text = remove_stopwords(text, lang=lang)\n",
    "    if apply_stem:\n",
    "        text = apply_stemming(text)\n",
    "    if apply_lemma:\n",
    "        text = apply_lemmatization(text)\n",
    "    return text\n",
    "\n",
    "df=pd.read_csv(\"data.csv\")\n",
    "df = df.sample(n=10000, random_state=42)  # Sadece 10.000 satır kullan\n",
    "\n",
    "# Özellikler ve etiketleri ayır\n",
    "df['processed_review'] = df['review'].apply(lambda x: preprocess_text(x))\n",
    "# Örnek veri: Veri setini yükleme (örneğin, df['review'] ve df['sentiment'])\n",
    "# Varsayılan veri formatı\n",
    "texts = df['review'].apply(preprocess_text)  # Yorum metinleri\n",
    "labels = df['sentiment'].apply(preprocess_text)  # Etiketler (0: olumsuz, 1: olumlu)\n",
    "\n",
    "# Etiketleri sayısal değerlere dönüştürme\n",
    "labels = labels.map({'positive': 1, 'negative': 0})\n",
    "\n",
    "# Eğitim ve test setine ayırma\n",
    "X_train, X_test, y_train, y_test = train_test_split(texts, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "# Bag-of-Words (BoW) ile temsil max features üreteceği max kelime sayısı\n",
    "bow_vectorizer = CountVectorizer(max_features=5000, ngram_range=(1, 2))  # Unigram ve Bigram\n",
    "X_train_bow = bow_vectorizer.fit_transform(X_train)\n",
    "X_test_bow = bow_vectorizer.transform(X_test)\n",
    "\n",
    "# TF-IDF ile temsil\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))  # Unigram ve Bigram\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "\n",
    "# SVM Modeli (Bag-of-Words ile)\n",
    "svm_bow = SVC(kernel='rbf',C=1, degree=2, gamma='scale', random_state=42)\n",
    "svm_bow.fit(X_train_bow, y_train)\n",
    "y_pred_bow = svm_bow.predict(X_test_bow)\n",
    "accuracy_bow = accuracy_score(y_test, y_pred_bow)\n",
    "\n",
    "# SVM Modeli (TF-IDF ile)\n",
    "svm_tfidf = SVC(kernel='rbf',C=1, degree=2, gamma='scale', random_state=42)\n",
    "svm_tfidf.fit(X_train_tfidf, y_train)\n",
    "y_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
    "accuracy_tfidf = accuracy_score(y_test, y_pred_tfidf)\n",
    "\n",
    "# Sonuçları görüntüleme\n",
    "bow_report = classification_report(y_test, y_pred_bow)\n",
    "tfidf_report = classification_report(y_test, y_pred_tfidf)\n",
    "\n",
    "# Raporları ve doğrulukları yazdır\n",
    "print(f\"Bag-of-Words SVM Accuracy: {accuracy_bow:.2f}\")\n",
    "print(\"Bag-of-Words Classification Report:\")\n",
    "print(bow_report)\n",
    "\n",
    "print(f\"TF-IDF SVM Accuracy: {accuracy_tfidf:.2f}\")\n",
    "print(\"TF-IDF Classification Report:\")\n",
    "print(tfidf_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47f18357-6153-4815-9632-f1b983691f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['models/svm/svm_model_bow.joblib']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Model ve vektörleştiricileri kaydet\n",
    "joblib.dump(svm_tfidf, 'models/svm/svm_model_tfidf.joblib')\n",
    "joblib.dump(tfidf_vectorizer, 'models/svm/svm_tfidf_vectorizer.joblib')\n",
    "\n",
    "# Eğer Bag-of-Words modeli de kayıt edilecekse\n",
    "joblib.dump(svm_bow, 'models/svm/svm_model_bow.joblib')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79a30d31-4d98-40f9-be4a-81673284941d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tahmin Sonucu: [0]\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# Kayıtlı model ve vektörleştiricileri yükle\n",
    "loaded_svm_model = joblib.load('models/svm/svm_model_tfidf.joblib')\n",
    "loaded_tfidf_vectorizer = joblib.load('models/svm/svm_tfidf_vectorizer.joblib')\n",
    "\n",
    "# Yeni gelen bir yorum örneği\n",
    "new_review = \"dont watch this movie because it is waste of time\"\n",
    "\n",
    "# Ön işleme adımları\n",
    "processed_new_review = preprocess_text(new_review)  # preprocess_text fonksiyonunu tekrar kullanıyoruz\n",
    "\n",
    "# Veriyi vektörleştirme\n",
    "X_new = loaded_tfidf_vectorizer.transform([processed_new_review])\n",
    "\n",
    "# Tahmin\n",
    "prediction = loaded_svm_model.predict(X_new)\n",
    "print(\"Tahmin Sonucu:\", prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214d13df-ed46-4ed7-82f2-773f17dd7e60",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
